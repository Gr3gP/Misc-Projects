{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.Scikit build.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMKJkO6w5q5OHpxg6A7x6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gr3gP/Misc-Projects/blob/main/Naive_Bayes_Scikit_build.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1wB5doR3MWV",
        "outputId": "557486d5-5fa0-429b-9efd-f7d23c1a45c4"
      },
      "source": [
        "!pip install scratch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scratch\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/0c/b642ca6d51a61f941274dbb200a2b8032aa69cc38aae56182ca232af1caa/scratch-1.0.0.tar.gz\n",
            "Building wheels for collected packages: scratch\n",
            "  Building wheel for scratch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scratch: filename=scratch-1.0.0-py2.py3-none-any.whl size=4893 sha256=78c31b603fc377877dd50e91763599072c7a5d68e139ef7075fcb5c68e095082\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/38/12/319f86152c155f8113b6fad74d769defa09ba641717aba289b\n",
            "Successfully built scratch\n",
            "Installing collected packages: scratch\n",
            "Successfully installed scratch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPGWANfQzemC"
      },
      "source": [
        "from typing import Set\n",
        "import re\n",
        "\n",
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         # Convert to lowercase,\n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and\n",
        "    return set(all_words)                       # remove duplicates.\n",
        "\n",
        "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}\n",
        "\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Message(NamedTuple):\n",
        "    text: str\n",
        "    is_spam: bool\n",
        "\n",
        "from typing import List, Tuple, Dict, Iterable\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, k: float = 0.5) -> None:\n",
        "        self.k = k  # smoothing factor\n",
        "\n",
        "        self.tokens: Set[str] = set()\n",
        "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "        self.spam_messages = self.ham_messages = 0\n",
        "\n",
        "    def train(self, messages: Iterable[Message]) -> None:\n",
        "        for message in messages:\n",
        "            # Increment message counts\n",
        "            if message.is_spam:\n",
        "                self.spam_messages += 1\n",
        "            else:\n",
        "                self.ham_messages += 1\n",
        "\n",
        "            # Increment word counts\n",
        "            for token in tokenize(message.text):\n",
        "                self.tokens.add(token)\n",
        "                if message.is_spam:\n",
        "                    self.token_spam_counts[token] += 1\n",
        "                else:\n",
        "                    self.token_ham_counts[token] += 1\n",
        "\n",
        "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
        "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
        "        spam = self.token_spam_counts[token]\n",
        "        ham = self.token_ham_counts[token]\n",
        "\n",
        "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
        "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
        "\n",
        "        return p_token_spam, p_token_ham\n",
        "\n",
        "    def predict(self, text: str) -> float:\n",
        "        text_tokens = tokenize(text)\n",
        "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
        "\n",
        "        # Iterate through each word in our vocabulary.\n",
        "        for token in self.tokens:\n",
        "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
        "\n",
        "            # If *token* appears in the message,\n",
        "            # add the log probability of seeing it;\n",
        "            if token in text_tokens:\n",
        "                log_prob_if_spam += math.log(prob_if_spam)\n",
        "                log_prob_if_ham += math.log(prob_if_ham)\n",
        "\n",
        "            # otherwise add the log probability of _not_ seeing it\n",
        "            # which is log(1 - probability of seeing it)\n",
        "            else:\n",
        "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
        "\n",
        "        prob_if_spam = math.exp(log_prob_if_spam)\n",
        "        prob_if_ham = math.exp(log_prob_if_ham)\n",
        "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
        "\n",
        "messages = [Message(\"spam rules\", is_spam=True),\n",
        "            Message(\"ham rules\", is_spam=False),\n",
        "            Message(\"hello ham\", is_spam=False)]\n",
        "\n",
        "model = NaiveBayesClassifier(k=0.5)\n",
        "model.train(messages)\n",
        "\n",
        "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
        "assert model.spam_messages == 1\n",
        "assert model.ham_messages == 2\n",
        "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
        "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n",
        "\n",
        "text = \"hello spam\"\n",
        "\n",
        "probs_if_spam = [\n",
        "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n",
        "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
        "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
        "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n",
        "]\n",
        "\n",
        "probs_if_ham = [\n",
        "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n",
        "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
        "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
        "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n",
        "]\n",
        "\n",
        "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
        "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
        "\n",
        "# Should be about 0.83\n",
        "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7AbK7fsGvPF"
      },
      "source": [
        "#Gaussian NB with Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJwzhxc3_ZeF",
        "outputId": "647e0251-38b7-4fc8-fb26-bd592ca48f5a"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "\n",
        "#load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "#split into train adn test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "#instantiate model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#fit model and make prediction\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "print(\"Number of mislabeled points out of total %d points : %d\" \n",
        "      % (X_test.shape[0], (y_test != y_pred).sum()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of total 75 points : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oGs9OhKAa2"
      },
      "source": [
        "The GaussianNB assumes out data is distributed in a Gaussian. And although the NB classifier is considered fairly strong, it is a known weak estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adSOj6tI-dr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}